{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mdaffarudiyanto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mdaffarudiyanto/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import torch\n",
    "import json  \n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MT5ForConditionalGeneration\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/datasets/load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mdaffarudiyanto/GR_custom_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mdaffarudiyanto/GR_custom_dataset\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    \"mdaffarudiyanto/GR_custom_dataset\",\n",
    "    'stratified_articles_below_1000',\n",
    "    data_dir=\"/home/student/Documents/MDR/custom_liputan6_data\",\n",
    "    ignore_verifications=True\n",
    ")\n",
    "\n",
    "model_checkpoint = \"google/mt5-large\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_checkpoint, load_in_8bit = True, device_map = \"auto\")\n",
    "# model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "rouge_metric = load(\"rouge\")\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "prefix = \"summarize: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 75,497,472 || all params: 1,305,078,784 || trainable%: 5.7849\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=256,\n",
    "    lora_alpha=256,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"lora_only\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f11bfc8b3e84cdc87adfae53dd9f06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"clean_article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"clean_summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "    [(label if label != tokenizer.pad_token_id else -100) for label in labels_example]\n",
    "    for labels_example in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = f\"GR-{model_name}-LoRA-3-1\"\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  \n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "label_pad_token_id = -100\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=label_pad_token_id, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"]* 100,\n",
    "        \"rouge2\": rouge_result[\"rouge2\"]* 100,\n",
    "        \"rougeL\": rouge_result[\"rougeL\"]* 100,\n",
    "        \"rougeLsum\": rouge_result[\"rougeLsum\"]* 100\n",
    "    }\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db03a1a995d34c5797132631a9e8badf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8583, 'grad_norm': 0.5766535401344299, 'learning_rate': 9.860452135082335e-05, 'epoch': 0.07}\n",
      "{'loss': 1.5106, 'grad_norm': 0.6792715787887573, 'learning_rate': 9.720904270164667e-05, 'epoch': 0.14}\n",
      "{'loss': 1.4457, 'grad_norm': 0.5304698348045349, 'learning_rate': 9.581356405247e-05, 'epoch': 0.21}\n",
      "{'loss': 1.4192, 'grad_norm': 0.5255179405212402, 'learning_rate': 9.441808540329333e-05, 'epoch': 0.28}\n",
      "{'loss': 1.3752, 'grad_norm': 0.45272547006607056, 'learning_rate': 9.302260675411667e-05, 'epoch': 0.35}\n",
      "{'loss': 1.3625, 'grad_norm': 0.46935856342315674, 'learning_rate': 9.162712810494e-05, 'epoch': 0.42}\n",
      "{'loss': 1.3568, 'grad_norm': 0.4943619966506958, 'learning_rate': 9.023164945576334e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3299, 'grad_norm': 0.5023877024650574, 'learning_rate': 8.883617080658666e-05, 'epoch': 0.56}\n",
      "{'loss': 1.3232, 'grad_norm': 0.5666508078575134, 'learning_rate': 8.744069215741e-05, 'epoch': 0.63}\n",
      "{'loss': 1.3116, 'grad_norm': 0.7473479509353638, 'learning_rate': 8.604521350823333e-05, 'epoch': 0.7}\n",
      "{'loss': 1.3126, 'grad_norm': 0.4847542345523834, 'learning_rate': 8.464973485905666e-05, 'epoch': 0.77}\n",
      "{'loss': 1.2929, 'grad_norm': 0.40565794706344604, 'learning_rate': 8.325425620987999e-05, 'epoch': 0.84}\n",
      "{'loss': 1.292, 'grad_norm': 0.6634100675582886, 'learning_rate': 8.185877756070333e-05, 'epoch': 0.91}\n",
      "{'loss': 1.2879, 'grad_norm': 2.453195571899414, 'learning_rate': 8.046329891152665e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993cec7033674d3c8421b4ed9773dc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0792471170425415, 'eval_rouge1': 30.7314, 'eval_rouge2': 19.5819, 'eval_rougeL': 28.0315, 'eval_rougeLsum': 29.1585, 'eval_runtime': 1916.7673, 'eval_samples_per_second': 11.214, 'eval_steps_per_second': 0.467, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2792, 'grad_norm': 0.45638561248779297, 'learning_rate': 7.906782026235e-05, 'epoch': 1.05}\n",
      "{'loss': 1.2691, 'grad_norm': 0.5121920108795166, 'learning_rate': 7.767234161317332e-05, 'epoch': 1.12}\n",
      "{'loss': 1.2615, 'grad_norm': 1.1628835201263428, 'learning_rate': 7.627686296399665e-05, 'epoch': 1.19}\n",
      "{'loss': 1.257, 'grad_norm': 0.36726051568984985, 'learning_rate': 7.488138431481999e-05, 'epoch': 1.26}\n",
      "{'loss': 1.2664, 'grad_norm': 0.49681922793388367, 'learning_rate': 7.348590566564332e-05, 'epoch': 1.33}\n",
      "{'loss': 1.2532, 'grad_norm': 0.4368649125099182, 'learning_rate': 7.209042701646666e-05, 'epoch': 1.4}\n",
      "{'loss': 1.2503, 'grad_norm': 0.9414704442024231, 'learning_rate': 7.069494836728998e-05, 'epoch': 1.47}\n",
      "{'loss': 1.2436, 'grad_norm': 0.4505850076675415, 'learning_rate': 6.929946971811332e-05, 'epoch': 1.54}\n",
      "{'loss': 1.2328, 'grad_norm': 0.31621021032333374, 'learning_rate': 6.790399106893665e-05, 'epoch': 1.6}\n",
      "{'loss': 1.2374, 'grad_norm': 0.605460524559021, 'learning_rate': 6.650851241975998e-05, 'epoch': 1.67}\n",
      "{'loss': 1.2338, 'grad_norm': 0.47198286652565, 'learning_rate': 6.51130337705833e-05, 'epoch': 1.74}\n",
      "{'loss': 1.2376, 'grad_norm': 0.5009793639183044, 'learning_rate': 6.371755512140665e-05, 'epoch': 1.81}\n",
      "{'loss': 1.2238, 'grad_norm': 0.509995698928833, 'learning_rate': 6.232207647222997e-05, 'epoch': 1.88}\n",
      "{'loss': 1.2091, 'grad_norm': 0.5305375456809998, 'learning_rate': 6.092659782305331e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1507cd10a5413e893aeafe80d02897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0441831350326538, 'eval_rouge1': 31.1356, 'eval_rouge2': 19.9742, 'eval_rougeL': 28.4552, 'eval_rougeLsum': 29.5827, 'eval_runtime': 1922.3674, 'eval_samples_per_second': 11.181, 'eval_steps_per_second': 0.466, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2129, 'grad_norm': 0.5214790105819702, 'learning_rate': 5.9531119173876635e-05, 'epoch': 2.02}\n",
      "{'loss': 1.2196, 'grad_norm': 0.45561397075653076, 'learning_rate': 5.8135640524699976e-05, 'epoch': 2.09}\n",
      "{'loss': 1.2, 'grad_norm': 0.6206215023994446, 'learning_rate': 5.674016187552331e-05, 'epoch': 2.16}\n",
      "{'loss': 1.2091, 'grad_norm': 0.39133739471435547, 'learning_rate': 5.534468322634664e-05, 'epoch': 2.23}\n",
      "{'loss': 1.2014, 'grad_norm': 0.5446147918701172, 'learning_rate': 5.394920457716998e-05, 'epoch': 2.3}\n",
      "{'loss': 1.2043, 'grad_norm': 0.5717746615409851, 'learning_rate': 5.2553725927993305e-05, 'epoch': 2.37}\n",
      "{'loss': 1.2047, 'grad_norm': 0.7855800986289978, 'learning_rate': 5.115824727881664e-05, 'epoch': 2.44}\n",
      "{'loss': 1.197, 'grad_norm': 0.4432491958141327, 'learning_rate': 4.9762768629639966e-05, 'epoch': 2.51}\n",
      "{'loss': 1.1849, 'grad_norm': 0.41299527883529663, 'learning_rate': 4.8367289980463307e-05, 'epoch': 2.58}\n",
      "{'loss': 1.1838, 'grad_norm': 0.4671424627304077, 'learning_rate': 4.6971811331286634e-05, 'epoch': 2.65}\n",
      "{'loss': 1.1906, 'grad_norm': 0.5052557587623596, 'learning_rate': 4.557633268210997e-05, 'epoch': 2.72}\n",
      "{'loss': 1.1853, 'grad_norm': 0.4934720993041992, 'learning_rate': 4.41808540329333e-05, 'epoch': 2.79}\n",
      "{'loss': 1.1873, 'grad_norm': 0.4076961278915405, 'learning_rate': 4.278537538375663e-05, 'epoch': 2.86}\n",
      "{'loss': 1.1914, 'grad_norm': 0.45151710510253906, 'learning_rate': 4.138989673457996e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f929d31217f4f738329d2ec6df7f5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.024794340133667, 'eval_rouge1': 31.0846, 'eval_rouge2': 19.9549, 'eval_rougeL': 28.3784, 'eval_rougeLsum': 29.494, 'eval_runtime': 1921.5781, 'eval_samples_per_second': 11.186, 'eval_steps_per_second': 0.466, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1896, 'grad_norm': 0.4501349925994873, 'learning_rate': 3.99944180854033e-05, 'epoch': 3.0}\n",
      "{'loss': 1.1775, 'grad_norm': 0.41838252544403076, 'learning_rate': 3.859893943622663e-05, 'epoch': 3.07}\n",
      "{'loss': 1.1752, 'grad_norm': 0.449942946434021, 'learning_rate': 3.720346078704996e-05, 'epoch': 3.14}\n",
      "{'loss': 1.1711, 'grad_norm': 0.4604440927505493, 'learning_rate': 3.580798213787329e-05, 'epoch': 3.21}\n",
      "{'loss': 1.1721, 'grad_norm': 0.47075650095939636, 'learning_rate': 3.4412503488696626e-05, 'epoch': 3.28}\n",
      "{'loss': 1.1735, 'grad_norm': 0.43812647461891174, 'learning_rate': 3.301702483951995e-05, 'epoch': 3.35}\n",
      "{'loss': 1.1649, 'grad_norm': 0.4246861934661865, 'learning_rate': 3.162154619034329e-05, 'epoch': 3.42}\n",
      "{'loss': 1.1681, 'grad_norm': 0.4019034802913666, 'learning_rate': 3.022606754116662e-05, 'epoch': 3.49}\n",
      "{'loss': 1.169, 'grad_norm': 0.40389928221702576, 'learning_rate': 2.883058889198995e-05, 'epoch': 3.56}\n",
      "{'loss': 1.1523, 'grad_norm': 0.4047265350818634, 'learning_rate': 2.743511024281329e-05, 'epoch': 3.63}\n",
      "{'loss': 1.1705, 'grad_norm': 0.40151217579841614, 'learning_rate': 2.603963159363662e-05, 'epoch': 3.7}\n",
      "{'loss': 1.1627, 'grad_norm': 0.42270708084106445, 'learning_rate': 2.464415294445995e-05, 'epoch': 3.77}\n",
      "{'loss': 1.161, 'grad_norm': 0.454140841960907, 'learning_rate': 2.324867429528328e-05, 'epoch': 3.84}\n",
      "{'loss': 1.157, 'grad_norm': 0.4507802724838257, 'learning_rate': 2.1853195646106618e-05, 'epoch': 3.91}\n",
      "{'loss': 1.1595, 'grad_norm': 0.4382227659225464, 'learning_rate': 2.0457716996929948e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3706edf998145999bd4a0cc3779b4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0112848281860352, 'eval_rouge1': 31.3254, 'eval_rouge2': 20.1852, 'eval_rougeL': 28.6607, 'eval_rougeLsum': 29.7518, 'eval_runtime': 1921.2515, 'eval_samples_per_second': 11.187, 'eval_steps_per_second': 0.466, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1493, 'grad_norm': 0.5039042830467224, 'learning_rate': 1.9062238347753282e-05, 'epoch': 4.05}\n",
      "{'loss': 1.1436, 'grad_norm': 0.43603894114494324, 'learning_rate': 1.7666759698576613e-05, 'epoch': 4.12}\n",
      "{'loss': 1.1337, 'grad_norm': 0.41771361231803894, 'learning_rate': 1.6271281049399943e-05, 'epoch': 4.19}\n",
      "{'loss': 1.1543, 'grad_norm': 0.49262088537216187, 'learning_rate': 1.4875802400223277e-05, 'epoch': 4.26}\n",
      "{'loss': 1.1488, 'grad_norm': 0.471996933221817, 'learning_rate': 1.3480323751046608e-05, 'epoch': 4.33}\n",
      "{'loss': 1.1453, 'grad_norm': 0.4908730089664459, 'learning_rate': 1.2084845101869942e-05, 'epoch': 4.4}\n",
      "{'loss': 1.1438, 'grad_norm': 0.4520229697227478, 'learning_rate': 1.0689366452693274e-05, 'epoch': 4.47}\n",
      "{'loss': 1.1527, 'grad_norm': 0.5432513952255249, 'learning_rate': 9.293887803516608e-06, 'epoch': 4.54}\n",
      "{'loss': 1.1536, 'grad_norm': 0.47976547479629517, 'learning_rate': 7.898409154339938e-06, 'epoch': 4.61}\n",
      "{'loss': 1.1553, 'grad_norm': 0.4538290202617645, 'learning_rate': 6.502930505163271e-06, 'epoch': 4.67}\n",
      "{'loss': 1.1532, 'grad_norm': 0.4502216875553131, 'learning_rate': 5.107451855986604e-06, 'epoch': 4.74}\n",
      "{'loss': 1.1383, 'grad_norm': 0.43507301807403564, 'learning_rate': 3.7119732068099357e-06, 'epoch': 4.81}\n",
      "{'loss': 1.1485, 'grad_norm': 0.5040686130523682, 'learning_rate': 2.3164945576332683e-06, 'epoch': 4.88}\n",
      "{'loss': 1.1412, 'grad_norm': 0.46137258410453796, 'learning_rate': 9.210159084566007e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/student/Documents/MDR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f56269312cd4acf8d6a13389010c977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/896 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0041303634643555, 'eval_rouge1': 31.4683, 'eval_rouge2': 20.255, 'eval_rougeL': 28.7584, 'eval_rougeLsum': 29.8822, 'eval_runtime': 1913.1327, 'eval_samples_per_second': 11.235, 'eval_steps_per_second': 0.468, 'epoch': 5.0}\n",
      "{'train_runtime': 94123.6574, 'train_samples_per_second': 9.135, 'train_steps_per_second': 0.381, 'train_loss': 1.2565770755291916, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35830, training_loss=1.2565770755291916, metrics={'train_runtime': 94123.6574, 'train_samples_per_second': 9.135, 'train_steps_per_second': 0.381, 'total_flos': 2.770680372485161e+18, 'train_loss': 1.2565770755291916, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to GR-mt5-large-LoRA-3-1/best_model\n"
     ]
    }
   ],
   "source": [
    "best_model_path = os.path.join(output_dir, \"best_model\")\n",
    "trainer.save_model(best_model_path)\n",
    "print(f\"Best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = \"mT5-large_LoRA_3-1_results\"\n",
    "trainer.model.save_pretrained(peft_model_id)\n",
    "tokenizer.save_pretrained(peft_model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATE SUMMARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdaffarudiyanto/Documents/Graduation Research/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/home/mdaffarudiyanto/Documents/Graduation Research/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/mdaffarudiyanto/Documents/Graduation Research/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = \"mT5-small_LoRA_3_results\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,  load_in_8bit=True,  device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    " \n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
    "model.eval()\n",
    " \n",
    "print(\"Peft model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdaffarudiyanto/Documents/Graduation Research/.venv/lib/python3.10/site-packages/datasets/load.py:2524: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "/home/mdaffarudiyanto/Documents/Graduation Research/.venv/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mdaffarudiyanto/GR_custom_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mdaffarudiyanto/GR_custom_dataset\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f629d7f278b4e52a4fdfb3a7bfdf160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7270ab4a8d4cbdae0184554cd4582e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f812b2fa8db849228a454bb196f1104d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1122d71e7ff6402ebd9f42c980520471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb7bfad78b342f8861d0c3b13c53e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11acf8821964485bae1559254d187f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171963 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    \"mdaffarudiyanto/GR_custom_dataset\",\n",
    "    'stratified_articles_below_1000',\n",
    "    data_dir=\"custom_liputan6_data\",\n",
    "    ignore_verifications=True\n",
    ")\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"clean_article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"clean_summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "  \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "    [(label if label != tokenizer.pad_token_id else -100) for label in labels_example]\n",
    "    for labels_example in model_inputs[\"labels\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    articles = [item['clean_article'] for item in batch]\n",
    "    summaries = [item['clean_summary'] for item in batch]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        articles,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    return inputs, articles, summaries\n",
    "\n",
    "batch_size = 64\n",
    "test_dataloader = DataLoader(\n",
    "    raw_datasets[\"test\"],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/336 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [55:09<00:00,  9.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c497bb940b43d9a4ff6fcfe8b3a7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f049cbc7a8e4cd185c232584b736cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 70.03 seconds, 307.02 sentences/sec\n",
      "Average Precision: 78.31\n",
      "Average Recall: 78.23\n",
      "Average F1 Score: 78.22\n",
      "ROUGE Scores:\n",
      "ROUGE-1 F1 Score: 43.77\n",
      "ROUGE-2 F1 Score: 28.97\n",
      "ROUGE-L F1 Score: 37.99\n",
      "ROUGE-Lsum F1 Score: 37.99\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 96\u001b[0m\n\u001b[1;32m     87\u001b[0m summary_comparison_df \u001b[38;5;241m=\u001b[39m generate_and_compare_summaries(\n\u001b[1;32m     88\u001b[0m     model,\n\u001b[1;32m     89\u001b[0m     tokenizer,\n\u001b[1;32m     90\u001b[0m     test_dataloader,\n\u001b[1;32m     91\u001b[0m     device\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m summary_comparison_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoRA_summary_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 96\u001b[0m display(\u001b[43mHTML\u001b[49m(summary_comparison_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mto_html(escape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_and_compare_summaries(model, tokenizer, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    display_data = []\n",
    "    all_generated_summaries = []\n",
    "    all_reference_summaries = []\n",
    "\n",
    "    for inputs, articles, summaries in tqdm(dataloader, desc=\"Processing\"):\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_target_length,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        generated_summaries = tokenizer.batch_decode(\n",
    "            summary_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        for article, reference_summary, generated_summary in zip(articles, summaries, generated_summaries):\n",
    "            display_data.append({\n",
    "                \"Original Text\": article,\n",
    "                \"Reference Summary\": reference_summary,\n",
    "                \"Generated Summary\": generated_summary\n",
    "            })\n",
    "            all_generated_summaries.append(generated_summary)\n",
    "            all_reference_summaries.append(reference_summary)\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        all_generated_summaries,\n",
    "        all_reference_summaries,\n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        num_layers=9,\n",
    "        lang='id',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    avg_precision = P.mean().item() * 100\n",
    "    avg_recall = R.mean().item() * 100\n",
    "    avg_f1 = F1.mean().item() * 100\n",
    "\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    rouge_scores = rouge_metric.compute(\n",
    "        predictions=all_generated_summaries,\n",
    "        references=all_reference_summaries,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "    \n",
    "    rouge1 = rouge_scores['rouge1'] * 100\n",
    "    rouge2 = rouge_scores['rouge2'] * 100\n",
    "    rougeL = rouge_scores['rougeL'] * 100\n",
    "    rougeLsum = rouge_scores['rougeLsum'] * 100\n",
    "    \n",
    "    print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.2f}\")\n",
    "    \n",
    "    print(f\"ROUGE Scores:\")\n",
    "    print(f\"ROUGE-1 F1 Score: {rouge1:.2f}\")\n",
    "    print(f\"ROUGE-2 F1 Score: {rouge2:.2f}\")\n",
    "    print(f\"ROUGE-L F1 Score: {rougeL:.2f}\")\n",
    "    print(f\"ROUGE-Lsum F1 Score: {rougeLsum:.2f}\")\n",
    "     \n",
    "    scores_dict = {\n",
    "        'BERTScore': {\n",
    "            'Precision': avg_precision,\n",
    "            'Recall': avg_recall,\n",
    "            'F1': avg_f1\n",
    "        },\n",
    "        'ROUGE': {\n",
    "            'ROUGE-1': rouge1,\n",
    "            'ROUGE-2': rouge2,\n",
    "            'ROUGE-L': rougeL,\n",
    "            'ROUGE-Lsum': rougeLsum\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('evaluation_scores.json', 'w') as f:\n",
    "        json.dump(scores_dict, f, indent=4)\n",
    "\n",
    "    return pd.DataFrame(display_data)\n",
    "\n",
    "summary_comparison_df = generate_and_compare_summaries(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    test_dataloader,\n",
    "    device\n",
    ")\n",
    "\n",
    "summary_comparison_df.to_csv(\"LoRA_summary_results.csv\", index=False)\n",
    "\n",
    "display(HTML(summary_comparison_df.head(30).to_html(escape=False)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
